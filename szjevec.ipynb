{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mediapipe opencv-python numpy scikit-learn tensorflow\n",
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PATH = \"videos\"\n",
    "DATASET_PATH = \"dataset\"\n",
    "\n",
    "# Whether to do processing in the background, which is faster,\n",
    "# but you don't see the landmarks while processing\n",
    "BACKGROUND = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import xgboost as xgb\n",
    "\n",
    "from keras import layers, Sequential\n",
    "from keras.callbacks import TensorBoard, EarlyStopping\n",
    "from keras.optimizers import AdamW\n",
    "from keras.utils import to_categorical, pad_sequences\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    out = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, out\n",
    "\n",
    "\n",
    "def get_landmarks(detection):\n",
    "    return (\n",
    "        # Since we use just one hand at a time, the first hand [0] is the only one we need\n",
    "        np.array(\n",
    "            [[l.x, l.y, l.z] for l in detection.multi_hand_landmarks[0].landmark]\n",
    "        ).flatten()\n",
    "        if detection.multi_hand_landmarks\n",
    "        else np.zeros(21 * 3)  # 21 landmarks with 3 coordinates\n",
    "    )\n",
    "\n",
    "\n",
    "def get_video_filenames(path: str) -> dict[str, list[str]]:\n",
    "    if not os.path.isdir(path):\n",
    "        raise Exception(f\"{path} is not a valid directory!\")\n",
    "\n",
    "    gestures = dict()\n",
    "    for directory in os.listdir(path):\n",
    "        if not os.path.isdir(os.path.join(path, directory)):\n",
    "            continue\n",
    "\n",
    "        gestures[directory] = []\n",
    "        for file in os.listdir(os.path.join(path, directory)):\n",
    "            if not file.endswith(\".mp4\"):\n",
    "                continue\n",
    "\n",
    "            gestures[directory].append(os.path.join(path, directory, file))\n",
    "\n",
    "    return gestures\n",
    "\n",
    "\n",
    "def cvexit(vid):\n",
    "    vid.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    raise SystemExit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp_hands.Hands(\n",
    "    min_detection_confidence=0.8,\n",
    "    min_tracking_confidence=0.5,\n",
    "    max_num_hands=1,\n",
    ") as hands:\n",
    "    filenames = get_video_filenames(VIDEO_PATH)\n",
    "    for gesture in filenames:\n",
    "        for filename in filenames[gesture]:\n",
    "            # Skip already processed videos\n",
    "            if os.path.exists(\n",
    "                os.path.join(\n",
    "                    DATASET_PATH, gesture, pathlib.Path(filename).stem + \".npy\"\n",
    "                )\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            cap = cv2.VideoCapture(filename)\n",
    "            sequences = []\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                img, res = mediapipe_detection(frame, hands)\n",
    "                landmarks = get_landmarks(res)\n",
    "                sequences.append(landmarks)\n",
    "\n",
    "                if not BACKGROUND:\n",
    "                    mp_drawing.draw_landmarks(img, landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                    cv2.imshow(\"Landmark detection\", img)\n",
    "\n",
    "                    # Exit if pressing esc\n",
    "                    if cv2.waitKey(1) == 27:\n",
    "                        cvexit(cap)\n",
    "\n",
    "            gesture_dir = os.path.join(DATASET_PATH, gesture)\n",
    "            if not os.path.isdir(gesture_dir):\n",
    "                os.makedirs(gesture_dir, exist_ok=True)\n",
    "\n",
    "            out_filename = pathlib.Path(filename).stem + \".npy\"\n",
    "            np.save(os.path.join(gesture_dir, out_filename), np.array(sequences))\n",
    "            print(\"Processed\", os.path.join(gesture, out_filename))\n",
    "\n",
    "# Cap will not be defined if there were no videos to process\n",
    "try:\n",
    "    cap.release()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_arrays(path):\n",
    "    if not os.path.isdir(path):\n",
    "        raise Exception(f\"{path} is not a valid directory!\")\n",
    "\n",
    "    # First pass is just to get the label names (letters)\n",
    "    label_map = dict()\n",
    "    for letter in os.listdir(path):\n",
    "        if not os.path.isdir(os.path.join(path, letter)):\n",
    "            continue\n",
    "\n",
    "        label_map[letter] = len(label_map)\n",
    "\n",
    "    # Second pass is to get the actual data\n",
    "    sequences, labels = [], []\n",
    "    for letter in os.listdir(path):\n",
    "        if not os.path.isdir(os.path.join(path, letter)):\n",
    "            continue\n",
    "\n",
    "        for file in os.listdir(os.path.join(path, letter)):\n",
    "            if not file.endswith(\".npy\"):\n",
    "                continue\n",
    "\n",
    "            sequences.append(np.load(os.path.join(path, letter, file)))\n",
    "            labels.append(label_map[letter])\n",
    "\n",
    "    return label_map, np.array(labels), sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map, labels, sequences = get_dataset_arrays(DATASET_PATH)\n",
    "labels = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_size = 30  # Size in frames\n",
    "\n",
    "split_seqs = []\n",
    "split_labels = []\n",
    "\n",
    "if win_size > 0:\n",
    "    for seq, label in zip(sequences, labels):\n",
    "        for i in range(0, len(seq), win_size):\n",
    "            split_seqs.append(seq[i : i + win_size])\n",
    "            split_labels.append(label)\n",
    "\n",
    "    # We need to pad all sequences to the same length (length of the longest sequence)\n",
    "    # The smaller the window size, the less padding will be added\n",
    "    split_seqs = pad_sequences(split_seqs)\n",
    "    split_labels = np.array(split_labels)\n",
    "else:\n",
    "    split_seqs = pad_sequences(sequences)\n",
    "    split_labels = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seqs, test_seqs, train_labels, test_labels = [], [], [], []\n",
    "for letter in label_map:\n",
    "    _train_seqs, _test_seqs, _train_labels, _test_labels = train_test_split(\n",
    "        split_seqs, split_labels, test_size=0.2\n",
    "    )\n",
    "    train_seqs.append(_train_seqs)\n",
    "    test_seqs.append(_test_seqs)\n",
    "    train_labels.append(_train_labels)\n",
    "    test_labels.append(_test_labels)\n",
    "\n",
    "train_seqs = np.concatenate(train_seqs)\n",
    "test_seqs = np.concatenate(test_seqs)\n",
    "train_labels = np.concatenate(train_labels)\n",
    "test_labels = np.concatenate(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All labels:\", split_labels.shape)\n",
    "print(\"All sequences:\", split_seqs.shape)\n",
    "\n",
    "print(\"Train sequences:\", train_seqs.shape)\n",
    "print(\"Test sequences:\", test_seqs.shape)\n",
    "print(\"Train labels:\", train_labels.shape)\n",
    "print(\"Test labels:\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_indices = np.argmax(train_labels, axis=1)\n",
    "test_labels_indices = np.argmax(test_labels, axis=1)\n",
    "\n",
    "dtrain = xgb.DMatrix(\n",
    "    train_seqs.reshape(train_seqs.shape[0], -1), label=train_labels_indices\n",
    ")\n",
    "dtest = xgb.DMatrix(\n",
    "    test_seqs.reshape(test_seqs.shape[0], -1), label=test_labels_indices\n",
    ")\n",
    "\n",
    "params = {\n",
    "    \"max_depth\": 3,\n",
    "    \"eta\": 0.1,\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"num_class\": train_labels.shape[1],\n",
    "}\n",
    "\n",
    "num_round = 100  # number of training iterations\n",
    "bst = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "preds = bst.predict(dtest)\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds])\n",
    "\n",
    "print(\"Accuracy score:\", accuracy_score(test_labels_indices, best_preds))\n",
    "bst.save_model(\"xgb_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    layers=[\n",
    "        layers.LSTM(\n",
    "            units=64,\n",
    "            return_sequences=True,\n",
    "            activation=\"tanh\",\n",
    "            input_shape=(train_seqs.shape[1], train_seqs.shape[2]),\n",
    "        ),\n",
    "        layers.LSTM(units=128, return_sequences=True, activation=\"relu\"),\n",
    "        layers.LSTM(units=64, return_sequences=False, activation=\"relu\"),\n",
    "        layers.Dense(units=64, activation=\"relu\"),\n",
    "        layers.Dense(units=32, activation=\"relu\"),\n",
    "        layers.Dense(labels.shape[1], activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential(\n",
    "#     layers=[\n",
    "#         layers.LSTM(100, input_shape=(split_seqs.shape[1], split_seqs.shape[2])),\n",
    "#         layers.Dropout(0.5),\n",
    "#         layers.Dense(100, activation=\"relu\"),\n",
    "#         layers.Dense(labels.shape[1], activation=\"softmax\"),\n",
    "#     ]\n",
    "# )\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential(\n",
    "#     layers=[\n",
    "#         layers.Conv2D(\n",
    "#             filters=1,\n",
    "#             kernel_size=(2, 2),\n",
    "#             activation=\"relu\",\n",
    "#             padding=\"same\",\n",
    "#             input_shape=(split_seqs.shape[1], split_seqs.shape[2], 1),\n",
    "#         ),\n",
    "#         layers.MaxPooling2D((2, 2)),\n",
    "#         layers.Flatten(),\n",
    "#         layers.Dense(labels.shape[1], activation=\"softmax\"),\n",
    "#     ]\n",
    "# )\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW()\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"categorical_accuracy\"],\n",
    ")\n",
    "\n",
    "# Delete previous Tensorboard logs\n",
    "if os.path.isdir(\"train\"):\n",
    "    for file in os.listdir(\"train\"):\n",
    "        if file.startswith(\"events.out.tfevents\"):\n",
    "            os.remove(os.path.join(\"train\", file))\n",
    "\n",
    "model.fit(\n",
    "    train_seqs,\n",
    "    train_labels,\n",
    "    epochs=200,\n",
    "    callbacks=[TensorBoard(log_dir=\"\"), EarlyStopping(monitor=\"loss\")],\n",
    "    use_multiprocessing=True,\n",
    ")\n",
    "\n",
    "labels_hat = np.argmax(model.predict(test_seqs), axis=1).tolist()\n",
    "labels_true = np.argmax(test_labels, axis=1).tolist()\n",
    "\n",
    "print(multilabel_confusion_matrix(labels_true, labels_hat))\n",
    "print(\"Accuracy score: {:.3f}\".format(accuracy_score(labels_true, labels_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights\n",
    "model.save(\"model.keras\")\n",
    "\n",
    "# Convert the Keras model to TensorFlow Lite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS,\n",
    "]\n",
    "converter._experimental_lower_tensor_list_ops = False\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TensorFlow Lite model\n",
    "with open(\"app/src/main/assets/model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
